# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kvrp6frkn8rXQzZaovrna91Ns1j2foTs
"""

!pip install tensorflow
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Additional imports for evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

# Replace with your CSV file path
url = 'data.csv'
data = pd.read_csv(url)
print(data)

# Assuming intervals are from the 2nd to the second-last column
X = data.iloc[:, 1:-1]
# Assuming the pin type is in the last column
y = data.iloc[:, -1]

# Convert labels to numerical format if they are not already
label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}
y = y.map(label_mapping)

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

mu = scaler.mean_
sigma = scaler.scale_

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

mu, sigma

# ... [previous code to load and preprocess data] ...

# Define a Keras Sequential model with some modifications
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=X_train.shape[1:]),
    tf.keras.layers.Lambda(lambda x: (x - mu) / (sigma+1e-6)),
    tf.keras.layers.Dense(64, activation='relu'),  # Adjust the number of neurons
    tf.keras.layers.Dropout(0.1),  # Adjust dropout rate
    tf.keras.layers.Dense(32, activation='relu'),  # Additional layer
    tf.keras.layers.Dropout(0.1),  # Adjust dropout rate
    tf.keras.layers.Dense(16, activation='relu'),  # Additional layer
    tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')
])

# Compile the model with a different learning rate
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Early stopping with adjusted patience
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)

# Train the model with possibly adjusted batch size and epochs
history = model.fit(X_train, y_train, epochs=80, batch_size=16,
                    validation_split=0.2, callbacks=[early_stopping], verbose=1)

# ... [code to plot metrics and evaluate the model] ...

# Plot training & validation accuracy values
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Evaluate on test set
y_pred = np.argmax(model.predict(X_test), axis=1)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

probs = model.predict(X_train)

preds = np.argmax(probs, axis = -1)

np.min(probs[preds == 0][:, 0]), np.min(probs[preds == 1][:, 1]), np.min(probs[preds == 2][:, 2]), np.min(probs[preds == 3][:, 3]), np.min(probs[preds == 4][:, 4]),

preds == 2

cm = confusion_matrix(y_train, preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

def make_prediction(model, scaler, intervals):
    """
    Make a prediction based on input intervals.

    Parameters:
    model: The trained TensorFlow/Keras model
    scaler: The StandardScaler instance used for training data
    intervals: A list of interval values

    Returns:
    The predicted class
    """

    # Ensure the input is in the correct format (a 2D array)
    intervals_array = np.array([intervals])
    print(f"array: {intervals_array}")

    # Scale the intervals using the same scaler used for training
    # intervals_scaled = scaler.transform(intervals_array)
    # print(f"scaled: {intervals_scaled}")

    # Make a prediction
    prediction = model.predict(intervals_array)
    print(f"predictions: {prediction}")

    # Get the class with the highest probability
    predicted_class = np.argmax(prediction, axis=1)
    print(f"class: {predicted_class}")

    return predicted_class

# Example usage
custom_intervals = [187, 176, 209, 187]  # Replace with your intervals
predicted_class = make_prediction(model, scaler, custom_intervals)
# print("Predicted class:", predicted_class)

scaler.transform([[187, 176, 209, 187]])

# Convert to TFLite model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TFLite model
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

from google.colab import files
files.download('model.tflite')

